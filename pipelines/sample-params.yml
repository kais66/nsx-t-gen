# Default Sizing of NSX-T VMs
# Manager Sizing
# small  : 2 vCPU,  8GB RAM
# medium : 4 vCPU, 16GB RAM - default with OVA
# large  : 8 vCPU, 32GB RAM
# Controller Sizing
# default: 4 vCPU, 16GB RAM
# Edge Instance Sizing
# small  : 2 vCPU,  4GB RAM - Too small - dont use
# medium : 4 vCPU,  8GB RAM - default with OVA
# large  : 8 vCPU, 16GB RAM

# Controller is fixed to the 4 vCPU and 16 GB RAM
# Change default size for deployment for edge and mgr

enable_ansible_debug: false # set value to true for verbose output from ansible
nsx_t_installer: nsx-t-gen  # Set to name of installer or env or any value so resources can be identified

# vCenter details to deploy the Mgmt OVAs (Mgr, Edge, Controller)
vcenter_ip: 10.40.1.206
vcenter_username: administrator@vsphere.local
vcenter_password: "Admin!23"
vcenter_folder: 1-folder-1080
vcenter_datacenter: 1-datacenter-1080
vcenter_cluster: 1-cluster-729
vcenter_datastore: vdnetSharedStorage

# vcenter_manager: vcenter-1                # EDIT
# vcenter_rp:                               # EDIT - resource pool where mgmt plane vms would get deployed

# OVA general network settings
mgmt_portgroup: 'VM Network'
dns_server: 10.13.12.2
dns_domain: corp.local.io
ntp_servers: 10.13.12.2
default_gateway: 10.13.12.1
netmask: 255.255.255.0
# only NSX manager OVA is required
# path_to_ova: "http://build-squid.eng.vmware.com/build/mts/release/bora-8968195/publish/nsx-unified-appliance/exports/ova/"
nsx_image_webserver: "http://10.33.75.99:40001"
ova_file_name: "nsx-unified-appliance-2.2.0.0.1.8968271.ova"
ovftool_file_name: "VMware-ovftool-4.3.0-9158644-lin.x86_64.bundle"

nsx_manager_ip: 10.40.0.20
nsx_manager_username: admin
nsx_manager_password: Admin!23Admin
nsx_manager_assigned_hostname: "nsxt-manager-10" # Set as FQDN, will be used also as certificate common name
nsx_manager_root_pwd: VMware1!    # Min 8 chars, upper, lower, number, special digit
nsx_manager_deployment_size: small   # Recommended for real barebones demo, smallest setup

compute_manager_username: "Administrator@vsphere.local"
compute_manager_password: "Admin!23"

# For outbound uplink connection used by Edge
edge_uplink_profile_vlan: 0
# For internal overlay connection used by Esxi hosts
esxi_uplink_profile_vlan: 20

# Tunnel endpoint network ip pool
# change pool_end based on # of members in the tep pool
vtep_ip_pool_name: tep-ip-pool
vtep_ip_pool_cidr: 192.168.213.0/24
vtep_ip_pool_gateway: 192.168.213.1
vtep_ip_pool_start: 192.168.213.10
vtep_ip_pool_end: 192.168.213.200
#nsx_t_tep_pool_nameserver: 192.168.213.2 # Not required

## Controllers
controller_ips: 10.40.0.21
controller_default_gateway: 10.40.1.253
controller_ip_prefix_length: 23
controller_hostname_prefix: controller # no spaces, Generated name: controller_1.corp.local.io
controller_vmname_prefix: controller-vm # Generated edge host name would be "NSX-T controller-vm_1"
controller_cli_password: "Admin!23Admin" # Min 8 chars, upper, lower, num, special char
controller_root_password: "Admin!23Admin"
controller_deployment_size: "MEDIUM"
# where to create controllers
controller_compute_id: "domain-c25"
controller_storage_id: "datastore-30"
controller_management_network_id: "network-33"
controller_shared_secret: "Admin!23Admin"

## Edge nodes
edge_ips: 10.40.0.24 # comma separated ips, requires min 2 for HA
edge_default_gateway: 10.40.1.253
edge_ip_prefix_length: 23
edge_hostname_prefix: nsx-t-edge
edge_fabric_name_prefix: EdgeNode
edge_transport_node_prefix: edge-transp-node
edge_cli_password: "Admin!23Admin"
edge_root_password: "Admin!23Admin"
# Size of the edge determines size of loadbalancers and # of lbrs.
# So, choose the size of edge based on requirements
# edge_deployment_size: medium # Recommended for POCs, smaller setup (# of lbrs very limited)
# edge_deployment_size: large  # Recommended when 4 small lbrs are required
edge_deployment_size: "SMALL"
edge_compute_id: "domain-c25"
edge_storage_id: "datastore-30"
edge_data_network_id: "network-33"
edge_management_network_id: "network-33"

esx_ips: 10.40.1.211
esx_os_version: "6.5.0"
esx_root_password: "ca$hc0w"
esx_hostname_prefix: "esx-host"
esx_available_vmnic: "vmnic1"


nsx_t_edge_portgroup_ext: EDIT_ME_vlan-name-for-uplink     # For external routing
nsx_t_edge_portgroup_transport: EDIT_ME_vlan-name-for-TEP  # For TEP/overlay
# Edge network interfaces
# Network1 and Network4 are for mgmt and not used for uplink
# Network2 is for external uplink
# Network3 is for overlay
# Change only if necessary
nsx_t_edge_overlay_interface: fp-eth1 # Wired to Network3
nsx_t_edge_uplink_interface: fp-eth0  # Wired to Network2


# If ova deployment succeeded but controller membership failed or edges didnt get to join for any reason
# enable rerun of the configure controllers
rerun_configure_controllers: false  # set it to true if you want to rerun the configure controllers
                                    # (as part of base ova install job) even as ova deployment succeeded

# Memory reservation is turned ON by default with the NSX-T OVAs.
# This would mean a deployment of an edge or a mgr would reserve full memory
# leading to memory constraints
# if nsx_t_keep_reservation to true  - would keep reservation ON, recommended for production setups.
# if nsx_t_keep_reservation to false - would turn reservation OFF, recommended for POCs, smaller setups.
nsx_t_keep_reservation: true # for Prod setup



# For Edge External uplink
# Check with network admin if its tagged or untagged
nsx_t_transport_vlan: 0



# # For outbound uplink connection used by Edge
# nsx_t_single_uplink_profile_name: "single-uplink-profile"
# nsx_t_single_uplink_profile_mtu: 1600 # Min 1600
# nsx_t_single_uplink_profile_vlan: 0 # Default

# # For internal overlay connection used by Esxi hosts
# nsx_t_overlay_profile_name: "host-overlay-profile"
# nsx_t_overlay_profile_mtu: 1600 # Min 1600
# nsx_t_overlay_profile_vlan: EDIT_ME # VLAN ID for the TEP/Overlay network



############################
# MP resource spec
#############################

# Configs for T0Router (only one per run), T1Routers, Logical switches and tags...
# Make sure the ncp/cluster tag matches the one defined at the top level.
# Expects to use atleast 2 edge instances for HA that need to be installed
nsx_t_t0router_spec: |
  t0_router:
    name: DefaultT0Router
    ha_mode: 'ACTIVE_STANDBY'
    # Specify the edges to be used for hosting the T0Router instance
    edge_indexes:
      # Index starts from 1 -> denoting nsx-t-edge-01
      primary: 1   # Index for primary edge to be used
      secondary: 2 # Index for secondary edge to be used
    vip: 10.13.12.103/27
    ip1: 10.13.12.101/27
    ip2: 10.13.12.102/27
    vlan_uplink: 0
    static_route:
      next_hop: 10.13.12.1
      network: 0.0.0.0/0
      admin_distance: 1
    tags:
      ncp/cluster: 'pas1' # Should match the top level ncp/cluster tag value
      ncp/shared_resource: 'true' # required for PKS
      testtag: 'testpas1'


########### Following are what's used in
# T1 Logical Router with associated logical switches
# Add additional or comment off unnecessary t1 routers and switches as needed
# Can have 3 different setups:
# 1: One shared mgmt T1 Router and infra logical switch for both PKS & PAS
# 2: One mgmt T1 Router and infra logical switch for either PKS or PAS..
#    Comment off the T1 router not required
# 3: Separate mgmt T1 Router and infra logical switch for each PKS and PAS..
#    Add additional T1Router-Mgmt2 as needed with its infra logical switch
# Name the routers and logical switches and cidrs differently to avoid conflict

nsx_t_pas_ncp_cluster_tag: 'pas1'

nsx_t_t1router_logical_switches_spec: |
  t1_routers:
  # Add additional T1 Routers or collapse switches into same T1 Router as needed
  # Comment off the following T1 Routers if there is no PAS
  - name: T1-Router-PAS-Infra
    switches:
    - name: PAS-Infra
      logical_switch_gw: 192.168.10.1 # Last octet should be 1 rather than 0
      subnet_mask: 24

  - name: T1-Router-PAS-ERT
    switches:
    - name: PAS-ERT
      logical_switch_gw: 192.168.24.1 # Last octet should be 1 rather than 0
      subnet_mask: 24

  - name: T1-Router-PAS-Services
    switches:
    - name: PAS-Services
      logical_switch_gw: 192.168.28.1 # Last octet should be 1 rather than 0
      subnet_mask: 24

  - name: T1-Router-PAS-DynamicServices
    switches:
    - name: PAS-DynamicServices
      logical_switch_gw: 192.168.30.1 # Last octet should be 1 rather than 0
      subnet_mask: 24

  # Comment off the following T1 Routers if there is no PKS
  - name: T1-Router-PKS-Infra
    switches:
    - name: PKS-Infra
      logical_switch_gw: 192.168.20.1 # Last octet should be 1 rather than 0
      subnet_mask: 24

  - name: T1Router-PKS-K8s-Clusters
    switches:
    - name: PKS-K8s-Clusters
      logical_switch_gw: 192.168.40.1 # Last octet should be 1 rather than 0
      subnet_mask: 24


# Make sure the ncp/cluster tag matches the one defined on the T0 Router
# Additional the ncp/ha tag should be set for HA Spoof guard profile
nsx_t_ha_switching_profile_spec: |
  ha_switching_profiles:
  - name: HASwitchingProfile
    tags:
      ncp/cluster: 'pas1'  # Should match the top level ncp/cluster tag value
      ncp/ha: 'true'       # Required for HA
      testtag: 'testpas1'


# Make sure the ncp/cluster tag matches the one defined on the T0 Router
# Add additional container ip blocks as needed
nsx_t_container_ip_block_spec: |
  container_ip_blocks:
  - name: container-ip-block
    cidr: 10.4.0.0/16
    # Specify tags with PAS 2.0 and NSX Tile 2.1.0
    # Tags not needed for PAS 2.1 and NSX Tile 2.1.3
    tags:
      ncp/cluster: 'pas1' # Should match the top level ncp/cluster tag value
      testtag: 'testpas1'
  - name: node-container-ip-block-pks-with-tag
    cidr: 11.4.0.0/16
    ncp/shared_resource: 'true'
    # No tags for this block
  - name: pod-container-ip-block-pks-with-tag
    cidr: 12.4.0.0/16
    ncp/shared_resource: 'true'
    # No tags for this block



# Make sure the ncp/cluster tag matches the one defined on the T0 Router for PAS
# Make sure the ncp/shared tag is set to true for PKS
# Additional the ncp/external tag should be set for external facing ip pool
# Add additional exernal ip pools as needed
# Change the cidr, gateway, nameserver, dns_domain as needed
# The cidr, gateway, start/end ips should be reachable via static or bgp routing through T0 router
nsx_t_external_ip_pool_spec: |
  external_ip_pools:
  - name: snat-vip-pool-for-pas
    cidr: 10.100.0.0/24  # Should be a 0/24 or some valid cidr, matching the external exposed uplink
    gateway: 10.100.0.1
    start: 10.100.0.10 # Should not include gateway
    end: 10.100.0.200  # Should not include gateway
    nameserver: 10.100.0.2
    dns_domain: corp.local.io
    # Specify tags with PAS 2.0 and NSX Tile 2.1.0
    # Tags not needed for PAS 2.1 and NSX Tile 2.1.3
    tags:
      ncp/cluster: 'pas1'   # Should match the top level ncp/cluster tag value for PAS
      ncp/external: 'true'  # Required for external facing ips
      testtag: 'testpas1'
  - name: snat-vip-pool-with-no-tags
    cidr: 10.200.0.0/24  # Should be a 0/24 or some valid cidr, matching the external exposed uplink
    gateway: 10.200.0.1
    start: 10.200.0.10 # Should not include gateway
    end: 10.200.0.200  # Should not include gateway
    nameserver: 10.100.0.2
    dns_domain: corp.local.io    # No Tags needed for PKS
  - name: snat-vip-pool-for-pks
    cidr: 10.300.0.0/24 # Should be a 0/24 or some valid cidr, matching the external exposed uplink
    gateway: 10.300.0.1
    start: 10.300.0.10 # Should not include gateway
    end: 10.300.0.200  # Should not include gateway
    nameserver: 10.100.0.2
    dns_domain: corp.local.io
    tags:
      ncp/external: 'true'        # Required for external facing ips
      ncp/shared_resource: 'true'   # Required for PKS

# Specify NAT rules
# Provide matching dnat and snat rule for specific vms by using ips for destination_network and translated_network that need to be exposed like Ops Mgr
# Provide snat rules for outbound from either container or a vm by specifying the source_network (cidr) and translated network ip
# Ingress into Ops Mgr: External IP of Ops Mgr -> DNAT -> translated into internal ip of Ops Mgr
# Egress from Ops Mgr: internal ip of Ops Mgr -> SNAT -> translated into external IP of Ops Mgr
# Ingress into PKS API Controller: External IP of controller -> DNAT -> translated into internal ip of controller
# Egress from PKS API Controller: internal ip of controller -> SNAT -> translated into external IP of controller
# Egress from SomeNetwork: cidr of network -> SNAT -> translated into some external IP
nsx_t_nat_rules_spec: |
  nat_rules:
  # Sample entry for allowing inbound to PAS Ops manager - ingress
  - t0_router: DefaultT0Router
    nat_type: dnat
    destination_network: 10.13.12.2   # External IP address for PAS opsmanager
    translated_network: 192.168.10.2     # Internal IP of PAS Ops manager
    rule_priority: 1024                  # Higher priority
  # Sample entry for allowing outbound from PAS Ops Mgr to external - egress
  - t0_router: DefaultT0Router
    nat_type: snat
    source_network: 192.168.10.2         # Internal IP of PAS opsmanager
    translated_network: 10.13.12.2      # External IP address for PAS opsmanager
    rule_priority: 1024                  # Higher priority
  # Sample entry for PAS Infra network SNAT - egress
  - t0_router: DefaultT0Router
    nat_type: snat
    source_network: 192.168.10.0/24      # PAS Infra network cidr
    translated_network: 10.13.12.3      # SNAT External Address for PAS networks
    rule_priority: 8000                  # Lower priority

  # Sample entry for PAS ERT network SNAT - egress
  - t0_router: DefaultT0Router
    nat_type: snat
    source_network: 192.168.24.0/24      # PAS ERT network cidr
    translated_network: 10.13.12.3      # SNAT External Address for PAS networks
    rule_priority: 8000                  # Lower priority

  # Sample entry for PAS Services network SNAT - egress
  - t0_router: DefaultT0Router
    nat_type: snat
    source_network: 192.168.28.0/24      # PAS Services network cidr
    translated_network: 10.13.12.3      # SNAT External Address for PAS networks
    rule_priority: 8001                  # Lower priority

  # Sample entry for PAS Dynamic Services network SNAT - egress
  - t0_router: DefaultT0Router
    nat_type: snat
    source_network: 192.168.30.0/24      # PAS Dynamic Services network cidr
    translated_network: 10.13.12.3      # SNAT External Address for PAS networks
    rule_priority: 8001                  # Lower priority

  # Sample entry for PKS PKS-Infra network - egress
  - t0_router: DefaultT0Router
    nat_type: snat
    source_network: 192.168.20.0/24      # PKS Infra network cidr
    translated_network: 10.13.12.6       # SNAT External Address for PKS networks
    rule_priority: 8001                  # Lower priority

  # Sample entry for PKS PKS-Clusters network - egress
  - t0_router: DefaultT0Router
    nat_type: snat
    source_network: 192.168.40.0/24      # PKS Clusters network cidr
    translated_network: 10.13.12.6       # SNAT External Address for PKS networks
    rule_priority: 8001                  # Lower priority

  # Sample entry for allowing inbound to PKS Ops manager
  - t0_router: DefaultT0Router
    nat_type: dnat
    destination_network: 10.13.12.5      # External IP address for PKS opsmanager
    translated_network: 192.168.20.2     # Internal IP of PKS Ops manager
    rule_priority: 1024                  # Higher priority
  # Sample entry for allowing outbound from PKS Ops Mgr to external
  - t0_router: DefaultT0Router
    nat_type: snat
    source_network: 192.168.20.2         # Internal IP of PAS opsmanager
    translated_network: 10.13.12.5       # External IP address for PAS opsmanager
    rule_priority: 1024                  # Higher priority

  # Sample entry for allowing inbound to PKS Controller
  - t0_router: DefaultT0Router
    nat_type: dnat
    destination_network: 10.13.12.7      # External IP address for PKS controller
    translated_network: 192.168.20.4     # Internal IP of PKS Ops Controller
    rule_priority: 1024                  # Higher priority
  # Sample entry for allowing outbound from PKS Controller to external
  - t0_router: DefaultT0Router
    nat_type: snat
    source_network: 192.168.20.4        # Internal IP of PKS controller
    translated_network: 10.13.12.7      # External IP address for PKS controller
    rule_priority: 1024                 # Higher priority

nsx_t_csr_request_spec: |
  csr_request:
    #common_name not required - would use nsx_t_manager_host_name
    org_name: Company            # EDIT
    org_unit: net-integ          # EDIT
    country: US                  # EDIT
    state: CA                    # EDIT
    city: SF                     # EDIT
    key_size: 2048               # Valid values: 2048 or 3072
    algorithm: RSA               # Valid values: RSA or DSA

# LBR definition
# By default, all the server pools would use Layer 4 - TCP pass through
# No ssl termination or handling, uses default tcp active monitor & auto map for virtual server
# Auto Map mode uses LB interface IP and ephemeral port.
# In scenarios where both Clients and Pool Members are attached to the same Logical Router,
# SNAT (Auto Map or IP List) must be used.

# LBR Sizing guide
# LB Size | Virtual Servers | Pool Members
# small   |   10            |   30
# medium  |  100            |  300
# large   | 1000            | 3000

# No. of LBs per edge based on size of edge
# Edge Size | Small LBs| Medium LBs| Large LBs
#
# small     |    0     |   0       | 0
# medium    |    1     |   0       | 0 # Recommended for running only PAS or PKS
# large     |    4     |   1       | 1 # Recommended for running PAS + PKS
# Bare metal - not handled here

nsx_t_lbr_spec: |
  loadbalancers:
  # Sample entry for creating LBR for PAS ERT
  - name: PAS-ERT-LBR
    t1_router: T1-Router-PAS-ERT # Should match a previously declared T1 Router
    size: small                  # Allowed sizes: small, medium, large
    virtual_servers:
    - name: goRouter443          # Unique name that signifies function being exposed
      vip: 10.13.12.150          # Exposed VIP for LBR to listen on
      port: 443
      members:
      - ip: 192.168.24.31        # Internal ip of GoRouter instance 1
        port: 443
      - ip: 192.168.24.32        # Internal ip of GoRouter instance 2
        port: 443
    - name: goRouter80
      vip: 10.13.12.150
      port: 80
      members:
      - ip: 192.168.24.31        # Internal ip of GoRouter instance 1
        port: 80
      - ip: 192.168.24.32        # Internal ip of GoRouter instance 2
        port: 80
    - name: sshProxy             # SSH Proxy exposed to outside
      vip: 10.13.12.151
      port: 2222                 # Port 2222 for ssh proxy
      members:
      - ip: 192.168.24.41        # Internal ip of Diego Brain where ssh proxy runs
        port: 2222
